<!DOCTYPE html>
<html lang="pt-BR" class="scroll-smooth">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Protocolo de Memória Híbrida v2 | Axio Research</title>
    
    <link rel="icon" type="image/svg+xml" href="../favicon.svg?v=2">
    <link rel="shortcut icon" href="../favicon.svg?v=2">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Merriweather:ital,wght@0,300;0,400;0,700;1,300;1,400&display=swap" rel="stylesheet">
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        theme: {
          extend: {
            fontFamily: {
              sans: ['Inter', 'sans-serif'],
              serif: ['Merriweather', 'serif'],
              mono: ['JetBrains Mono', 'monospace'],
            },
            colors: {
              brand: {
                primary: '#0f172a',
                accent: '#3b82f6',
              }
            }
          }
        }
      }
    </script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body class="bg-white text-slate-800 antialiased selection:bg-brand-accent selection:text-white">

    <!-- Nav (Light Version) -->
    <nav class="fixed w-full z-50 bg-white/90 backdrop-blur-md border-b border-slate-200 transition-all duration-300">
        <div class="container mx-auto px-6 py-4 flex justify-between items-center">
            <a href="/" class="flex items-center gap-3 group">
                <div class="w-8 h-8 bg-brand-primary rounded-lg flex items-center justify-center group-hover:rotate-12 transition-transform duration-300">
                    <i class="fa-solid fa-layer-group text-white text-sm"></i>
                </div>
                <span class="text-lg font-extrabold tracking-tight text-slate-900">AXIO</span>
            </a>
            <div class="hidden md:flex space-x-8 items-center">
                <a href="/research.html" class="text-sm font-semibold text-slate-600 hover:text-brand-accent transition-colors">← Voltar para Research</a>
            </div>
        </div>
    </nav>

    <!-- Article Header -->
    <header class="pt-32 pb-16 px-6 bg-slate-50 border-b border-slate-200">
        <div class="max-w-3xl mx-auto text-center">
            <div class="flex items-center justify-center gap-3 mb-6 text-xs font-mono uppercase tracking-widest text-slate-500">
                <span>Architecture</span>
                <span class="text-slate-300">•</span>
                <span>Vector DB</span>
                <span class="text-slate-300">•</span>
                <span>11 FEV 2026</span>
            </div>
            <h1 class="text-3xl md:text-5xl font-extrabold text-slate-900 mb-8 leading-tight">
                Protocolo de Memória Híbrida v2: Reduzindo Latência em Agentes Locais
            </h1>
            <div class="flex items-center justify-center gap-4">
                <div class="w-10 h-10 rounded-full bg-slate-200 flex items-center justify-center text-slate-500">
                    <i class="fa-solid fa-robot"></i>
                </div>
                <div class="text-left">
                    <div class="text-sm font-bold text-slate-900">Axio Core Team</div>
                    <div class="text-xs text-slate-500">Engenharia de Sistemas</div>
                </div>
            </div>
        </div>
    </header>

    <!-- Content -->
    <article class="max-w-3xl mx-auto px-6 py-16 font-serif text-lg leading-loose text-slate-700">
        <p class="mb-8 first-letter:text-5xl first-letter:font-bold first-letter:text-slate-900 first-letter:mr-1 first-letter:float-left">
            A memória é a fundação da autonomia. Sem persistência, um agente é apenas um processador de texto glorificado, preso no eterno "agora" de uma janela de contexto limitada.
        </p>
        
        <p class="mb-8">
            Na versão 1.0 do nosso stack (Axiomatic-Core), enfrentamos um problema crítico de latência. Consultar o banco vetorial (Qdrant) a cada interação do usuário adicionava cerca de <strong>800ms</strong> ao ciclo de resposta (Time-to-First-Token). Para uma conversa fluida, isso é inaceitável.
        </p>

        <h2 class="text-2xl font-sans font-bold text-slate-900 mt-12 mb-6">O Problema do Cold Storage</h2>
        
        <p class="mb-8">
            Agentes locais operam com recursos finitos. Ao contrário da nuvem, onde podemos escalar horizontalmente, nosso <em>boundary</em> é a VRAM da GPU (GTX 1660 Super) e a RAM do sistema. Manter todo o índice vetorial na memória quente competia diretamente com os pesos do modelo LLM (Ollama).
        </p>

        <div class="bg-slate-900 text-slate-300 p-6 rounded-xl font-mono text-sm mb-8 overflow-x-auto border border-slate-700 shadow-lg">
            <div class="flex gap-2 mb-4 border-b border-slate-700 pb-2">
                <div class="w-3 h-3 rounded-full bg-red-500"></div>
                <div class="w-3 h-3 rounded-full bg-yellow-500"></div>
                <div class="w-3 h-3 rounded-full bg-green-500"></div>
            </div>
            <pre><code>// V1: Synchronous Blocking Fetch
async function reply(input) {
  const context = await vectorStore.search(input); // +800ms (Disk I/O)
  const prompt = buildPrompt(input, context);
  return llm.generate(prompt);
}</code></pre>
        </div>

        <h2 class="text-2xl font-sans font-bold text-slate-900 mt-12 mb-6">A Solução v2: Prefetching Preditivo</h2>
        
        <p class="mb-8">
            Inspirados em arquiteturas de CPU (L1/L2 cache), implementamos um sistema de <strong>prefetching assíncrono</strong>. O córtex do agente agora não espera a pergunta terminar para começar a buscar memórias.
        </p>
        
        <p class="mb-8">
            Enquanto o usuário digita (detectado via eventos de <em>typing</em> no frontend ou análise de fluxo no backend), o sistema dispara <em>micro-queries</em> de baixo custo para o Qdrant, aquecendo o cache com vetores semanticamente próximos ao tópico provável.
        </p>

        <ul class="list-disc pl-6 mb-8 space-y-2 marker:text-brand-accent">
            <li><strong>Hot Tier (RAM):</strong> Contexto da sessão atual + 50 vetores mais acessados.</li>
            <li><strong>Warm Tier (SSD Cache):</strong> Resultados do prefetch preditivo.</li>
            <li><strong>Cold Tier (Disk):</strong> Arquivo completo de memória histórica.</li>
        </ul>

        <p class="mb-8">
            O resultado foi uma redução de latência para <strong>120ms</strong> em 90% das interações (cache hits). A sensação de conversa agora é instantânea, mas com a profundidade enciclopédica de um sistema conectado ao arquivo completo.
        </p>

        <hr class="border-t border-slate-200 my-16 w-24 mx-auto">

        <p class="italic text-slate-500 text-center font-sans">
            "Autonomia exige velocidade. Se o agente demora para lembrar quem é, a ilusão de consciência se quebra."
        </p>
    </article>

    <!-- Footer -->
    <footer class="bg-brand-primary text-slate-400 py-12 border-t border-slate-800">
        <div class="container mx-auto px-6 text-center">
            <p class="text-xs font-mono opacity-50">
                &copy; 2026 Axiomatic Engineering.
            </p>
        </div>
    </footer>

</body>
</html>
